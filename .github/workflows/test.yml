name: Test and Validate

on:
  pull_request:
    branches: [ main ]
  push:
    branches: [ main ]

jobs:
  cli-tests:
    name: Test bp CLI
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Test CLI version
        run: |
          chmod +x cli/bp
          ./cli/bp --version

      - name: Test CLI list command
        run: ./cli/bp list terraform

      - name: Test CLI cache
        run: ./cli/bp cache stats

      - name: Build search index
        run: python3 -c "from cli.utils.search import build_search_index; build_search_index()"

      - name: Test search performance
        run: |
          python3 cli/utils/search.py
          echo "Search performance test completed"

      - name: Verify search performance target
        run: |
          python3 << 'EOF'
          from cli.utils.search import search_docs
          import time

          queries = ['kubernetes', 'docker postgres', 'terraform aws', 'ansible ubuntu']
          failed = False

          for query in queries:
              start = time.time()
              results = search_docs(query, limit=5)
              elapsed = (time.time() - start) * 1000

              print(f"Query '{query}': {len(results)} results in {elapsed:.2f}ms")

              if elapsed > 100:
                  print(f"FAIL: Search took {elapsed:.2f}ms, exceeds 100ms target")
                  failed = True

          if failed:
              exit(1)
          else:
              print("All searches completed within 100ms target")
          EOF

  documentation-tests:
    name: Test Documentation
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install MkDocs and dependencies
        run: |
          pip install -r requirements.txt

      - name: Validate MkDocs configuration
        run: mkdocs build --strict

      - name: Check for broken links in docs
        run: |
          find docs/ -name "*.md" -type f | while read file; do
            echo "Checking $file"
            if ! grep -oP '\[.*?\]\(\K[^)]+' "$file" > /dev/null 2>&1; then
              echo "No links found in $file (OK)"
            else
              grep -oP '\[.*?\]\(\K[^)]+' "$file" | while read link; do
                if [[ $link == http* ]]; then
                  echo "External link: $link (skipping in CI)"
                elif [[ $link == /* ]] || [[ $link == ../* ]]; then
                  echo "Relative link: $link (manual validation required)"
                fi
              done
            fi
          done

      - name: Verify all doc files exist
        run: |
          python3 << 'EOF'
          import yaml
          from pathlib import Path

          with open('mkdocs.yml', 'r') as f:
              config = yaml.safe_load(f)

          docs_dir = Path('docs')
          nav = config.get('nav', [])

          def check_nav_items(items, prefix=''):
              for item in items:
                  if isinstance(item, dict):
                      for key, value in item.items():
                          if isinstance(value, str):
                              doc_path = docs_dir / value
                              if not doc_path.exists():
                                  print(f"Missing: {doc_path}")
                                  exit(1)
                          elif isinstance(value, list):
                              check_nav_items(value, key)

          check_nav_items(nav)
          print("All documentation files exist")
          EOF

      - name: Upload documentation artifacts
        uses: actions/upload-artifact@v4
        with:
          name: documentation
          path: site/

  template-validation:
    name: Validate Templates
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Validate Terraform templates
        run: |
          for dir in terraform/*/; do
            if [ -f "$dir/main.tf" ] || [ -f "$dir/provider.tf" ]; then
              echo "Validating $dir"
              cd "$dir"
              terraform init -backend=false
              terraform validate
              terraform fmt -check
              cd - > /dev/null
            fi
          done

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'v1.28.0'

      - name: Validate Kubernetes manifests
        run: |
          for file in $(find kubernetes/ -name "*.yaml" -o -name "*.yml"); do
            echo "Validating $file"
            kubectl apply --dry-run=client -f "$file" || echo "Warning: $file validation failed (may need cluster context)"
          done

      - name: Validate Docker Compose files
        run: |
          for dir in docker-compose/*/; do
            if [ -f "$dir/compose.yaml" ] || [ -f "$dir/docker-compose.yml" ]; then
              echo "Validating $dir"
              docker compose -f "$dir/compose.yaml" config > /dev/null 2>&1 || \
              docker compose -f "$dir/docker-compose.yml" config > /dev/null 2>&1 || \
              echo "Warning: $dir validation failed"
            fi
          done

      - name: Setup Python for Ansible
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Ansible
        run: |
          pip install ansible ansible-lint

      - name: Validate Ansible playbooks
        run: |
          for file in $(find ansible/ -name "*.yaml" -o -name "*.yml"); do
            echo "Checking syntax: $file"
            ansible-playbook --syntax-check "$file" || echo "Warning: $file syntax check failed"
          done

  code-quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install linting tools
        run: |
          pip install flake8 black pylint yamllint

      - name: Check Python code formatting
        run: |
          black --check cli/ || echo "Run 'black cli/' to format code"

      - name: Lint Python code
        run: |
          flake8 cli/ --max-line-length=120 --ignore=E501,W503,E203 || echo "Linting warnings found"

      - name: Validate YAML files
        run: |
          yamllint -c .yamllint.yaml . || echo "YAML linting warnings found"

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
          exit-code: '0'

      - name: Upload Trivy results
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: 'trivy-results.sarif'

  snippets-validation:
    name: Validate Snippets
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install PyYAML

      - name: Validate snippet files
        run: |
          python3 << 'EOF'
          import yaml
          from pathlib import Path

          snippets_dir = Path('snippets')
          if not snippets_dir.exists():
              print("No snippets directory found")
              exit(0)

          for yaml_file in snippets_dir.glob('**/*.yaml'):
              print(f"Validating {yaml_file}")
              try:
                  with open(yaml_file, 'r') as f:
                      data = yaml.safe_load(f)

                  if data is None:
                      print(f"Warning: {yaml_file} is empty")
                      continue

                  if isinstance(data, dict) and 'snippets' in data:
                      for snippet in data['snippets']:
                          if 'name' not in snippet:
                              print(f"Error: Snippet missing 'name' in {yaml_file}")
                              exit(1)
                          if 'content' not in snippet:
                              print(f"Error: Snippet '{snippet.get('name')}' missing 'content' in {yaml_file}")
                              exit(1)

                  print(f"Valid: {yaml_file}")
              except yaml.YAMLError as e:
                  print(f"Error parsing {yaml_file}: {e}")
                  exit(1)

          print("All snippet files are valid")
          EOF

      - name: Test snippet loading
        run: |
          python3 << 'EOF'
          from cli.utils.snippets import load_snippets

          snippets = load_snippets()
          print(f"Loaded {len(snippets)} snippets")

          if len(snippets) == 0:
              print("Warning: No snippets loaded")
          else:
              print("Snippet loading successful")
              for name in list(snippets.keys())[:5]:
                  print(f"  - {name}")
          EOF

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [cli-tests, documentation-tests]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run full development setup
        run: |
          chmod +x scripts/install-bp.sh cli/bp
          python3 -c "from cli.utils.search import build_search_index; build_search_index()"
          python3 -c "from cli.utils.snippets import create_snippet_database; create_snippet_database()"

      - name: Test CLI workflow
        run: |
          ./cli/bp --version
          ./cli/bp list
          ./cli/bp cache stats
          echo "CLI workflow test completed"

      - name: Verify all components
        run: |
          echo "Checking directory structure..."
          test -d cli && echo "CLI: OK"
          test -d docs && echo "Docs: OK"
          test -d snippets && echo "Snippets: OK"
          test -f mkdocs.yml && echo "MkDocs config: OK"
          test -f requirements.txt && echo "Requirements: OK"
          test -f Makefile && echo "Makefile: OK"
          echo "All components verified"

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Benchmark search index build
        run: |
          python3 << 'EOF'
          from cli.utils.search import build_search_index
          import time

          start = time.time()
          index = build_search_index()
          elapsed = (time.time() - start) * 1000

          print(f"Index built in {elapsed:.2f}ms")
          print(f"Documents: {len(index['documents'])}")
          print(f"Templates: {len(index['templates'])}")
          print(f"Categories: {len(index['categories'])}")

          if elapsed > 1000:
              print(f"Warning: Index build took {elapsed:.2f}ms (target: <1000ms)")
          EOF

      - name: Benchmark search queries
        run: |
          python3 << 'EOF'
          from cli.utils.search import search_docs
          import time

          test_queries = [
              'kubernetes',
              'docker postgres',
              'terraform aws',
              'ansible ubuntu',
              'spring boot',
              'monitoring',
              'ci/cd',
              'nginx ingress'
          ]

          total_time = 0
          for query in test_queries:
              start = time.time()
              results = search_docs(query, limit=10)
              elapsed = (time.time() - start) * 1000
              total_time += elapsed

              print(f"Query '{query}': {len(results)} results in {elapsed:.2f}ms")

          avg_time = total_time / len(test_queries)
          print(f"\nAverage query time: {avg_time:.2f}ms")

          if avg_time > 50:
              print(f"Warning: Average query time {avg_time:.2f}ms exceeds 50ms")
          EOF

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [cli-tests, documentation-tests, template-validation, code-quality, snippets-validation, integration-tests, performance-tests]
    if: always()
    steps:
      - name: Check test results
        run: |
          echo "All tests completed"
          echo "CLI Tests: ${{ needs.cli-tests.result }}"
          echo "Documentation Tests: ${{ needs.documentation-tests.result }}"
          echo "Template Validation: ${{ needs.template-validation.result }}"
          echo "Code Quality: ${{ needs.code-quality.result }}"
          echo "Snippets Validation: ${{ needs.snippets-validation.result }}"
          echo "Integration Tests: ${{ needs.integration-tests.result }}"
          echo "Performance Tests: ${{ needs.performance-tests.result }}"

          if [ "${{ needs.cli-tests.result }}" != "success" ] || \
             [ "${{ needs.documentation-tests.result }}" != "success" ] || \
             [ "${{ needs.integration-tests.result }}" != "success" ]; then
            echo "Critical tests failed"
            exit 1
          fi

          echo "All critical tests passed"
